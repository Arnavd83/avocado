# Inference Server - Environment Variables
# Copy to .env and fill in your values
# IMPORTANT: Never commit .env to version control

# =============================================================================
# EXISTING PROJECT VARIABLES (from root .env)
# =============================================================================
# OpenRouter API Key
OPENROUTER_API_KEY=sk-or-v1-your_openrouter_key_here

# OpenAI Configuration (using OpenRouter)
OPENAI_API_KEY=sk-or-v1-your_openrouter_key_here
OPENAI_BASE_URL=https://openrouter.ai/api/v1

# OpenRouter App Identification
OPENROUTER_APP_NAME=avocado

# Tinker API key for finetuning
TINKER_API_KEY=your_tinker_api_key_here

# Lambda AI API KEY
LAMBDA_API_KEY=your_lambda_api_key_here

# HuggingFace API KEY
HUGGINGFACE_API_KEY=hf_your_huggingface_token_here

# =============================================================================
# INFERENCE SERVER - Lambda Cloud Configuration
# =============================================================================
# Lambda SSH key name (must be registered in Lambda Cloud)
LAMBDA_SSH_KEY_NAME=your_ssh_key_name

# Path to your private SSH key (for connecting to instances)
SSH_PRIVATE_KEY_PATH=~/.ssh/lambda_key

# Lambda persistent filesystem name
LAMBDA_FILESYSTEM_NAME=your_filesystem_name

# =============================================================================
# INFERENCE SERVER - Tailscale
# =============================================================================
# Tailscale auth key for joining the tailnet
# Generate at: https://login.tailscale.com/admin/settings/keys
# Recommended: use a reusable, ephemeral key
TS_AUTHKEY=tskey-auth-xxxxx

# =============================================================================
# INFERENCE SERVER - vLLM API Key
# =============================================================================
# Shared API key for vLLM endpoints - pick any string your team agrees on
# All researchers use the same key
VLLM_API_KEY=research-team-shared-key

# =============================================================================
# OPTIONAL - Overrides (uncomment to use)
# =============================================================================
# Override default model (must be defined in config/default.yaml)
# DEFAULT_MODEL=llama31-8b

# Override default GPU type
# DEFAULT_GPU=gpu_1x_a100

# Override idle shutdown timeout (minutes, 0 = disabled)
# IDLE_TIMEOUT=60

# Override health check timeout (seconds)
# HEALTH_CHECK_TIMEOUT=900
